{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18oIi2IERkM6AmVtCkUAfW6kPUdd9YgTD",
      "authorship_tag": "ABX9TyN/myUu5WCTOollciN1WH77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dml2611/HindiMRC/blob/main/automatic_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Rouge**"
      ],
      "metadata": {
        "id": "4py02B5pHYgi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ97Y2ovuHnO",
        "outputId": "e4d6b661-29a7-4cc8-f1c9-789d0899dc64"
      },
      "source": [
        "# Install rouge\n",
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3JGKfuMuA0z"
      },
      "source": [
        "# Import the libraries\n",
        "import pandas as pd\n",
        "from rouge import Rouge\n",
        "from csv import DictWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "1_cLOAqBLSfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ddd00f-857f-4c95-d851-385b54f8204b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the data input and output paths\n",
        "data_path = '/content/gdrive/MyDrive/Colab Notebooks/Lancaster University/GPT3.5 - Hindi MRC /'"
      ],
      "metadata": {
        "id": "WbK5QcxpyK4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_all = pd.read_excel(data_path + \"gpt_output.xlsx\", sheet_name='H4 GPT')\n",
        "gpt4_all = pd.read_excel(data_path + \"gpt_output.xlsx\", sheet_name='H4 GPT4')\n",
        "llama_all = pd.read_excel(data_path + \"gpt_output.xlsx\", sheet_name='H4 Llama')\n",
        "hindi_all = pd.read_excel(data_path + \"gpt_output.xlsx\", sheet_name='H4 Hindi GPT')\n",
        "\n",
        "reference = pd.read_excel(data_path + \"data.xlsx\" , sheet_name='Sheet2')"
      ],
      "metadata": {
        "id": "ZNaEDgjjypnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3 = gpt3_all['Answer 2']\n",
        "gpt4 = gpt4_all['Answer 4']\n",
        "llama = llama_all['Answer 1']\n",
        "hindi = hindi_all['Answer 3']\n",
        "\n",
        "ref = reference['Answer']"
      ],
      "metadata": {
        "id": "cAjzCvC_4DGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ref)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOabx3zp454B",
        "outputId": "bcf0bfb5-5cfd-4dc3-c6ec-051a4d8738ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for calculating the Rouge Scores\n",
        "def calc_rouge(answers, name):\n",
        "  rouge1, rouge2, rougeL = [], [], []\n",
        "  scores = rouge.get_scores(ref, answers, avg=True)\n",
        "  print(name)\n",
        "  rouge1_f1, rouge2_f1, rougeL_f1 = scores.get('rouge-1').get('f'), scores.get('rouge-2').get('f'), scores.get('rouge-l').get('f')\n",
        "  rouge1.append(rouge1_f1)\n",
        "  rouge2.append(rouge2_f1)\n",
        "  rougeL.append(rougeL_f1)\n",
        "  print(f'Rouge1-F1: {rouge1_f1}')\n",
        "  print(f'Rouge2-F1: {rouge2_f1}')\n",
        "  print(f'RougeL-F1:  {rougeL_f1}')\n",
        "  print(\"\\n\")\n",
        "  return scores"
      ],
      "metadata": {
        "id": "QqgrP3uYx02j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Rouge\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_gpt3 = calc_rouge(gpt3, \"GPT3.5\")\n",
        "rouge_gpt4 = calc_rouge(gpt4, \"GPT4\")\n",
        "rouge_hindi = calc_rouge(hindi, \"Hindi GPT\")\n",
        "rouge_llama  = calc_rouge(llama, \"Llama3\")"
      ],
      "metadata": {
        "id": "HSHQdrrM1swU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b692b7-b6fe-47db-b9ac-030bdd4926fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5\n",
            "Rouge1-F1: 0.5397915163553433\n",
            "Rouge2-F1: 0.4045104501297396\n",
            "RougeL-F1:  0.5101213236943313\n",
            "\n",
            "\n",
            "GPT4\n",
            "Rouge1-F1: 0.5115401653495586\n",
            "Rouge2-F1: 0.4011000523385893\n",
            "RougeL-F1:  0.4939012584615249\n",
            "\n",
            "\n",
            "Hindi GPT\n",
            "Rouge1-F1: 0.5404272927190086\n",
            "Rouge2-F1: 0.40420088456582226\n",
            "RougeL-F1:  0.5154521855716325\n",
            "\n",
            "\n",
            "Llama3\n",
            "Rouge1-F1: 0.5325626655749031\n",
            "Rouge2-F1: 0.4326076750201646\n",
            "RougeL-F1:  0.5164595383336685\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Bleu**"
      ],
      "metadata": {
        "id": "r7NBiT7aHgmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install nltk and bleu\n",
        "!pip install nltk\n",
        "!pip install bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B5V-Ww_oHkCo",
        "outputId": "3339b70f-d088-4ad0-b261-d5d0ce80a035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Collecting bleu\n",
            "  Downloading bleu-0.3.tar.gz (5.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficiency (from bleu)\n",
            "  Downloading efficiency-2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from efficiency->bleu) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from efficiency->bleu) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->efficiency->bleu) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->efficiency->bleu) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->efficiency->bleu) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->efficiency->bleu) (1.16.0)\n",
            "Downloading efficiency-2.0-py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: bleu\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-py3-none-any.whl size=5781 sha256=6cc83d3f770a3b3490c2cef9668592638c828edc05617bf0842baabb917a7d51\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/d8/d1/009abe01b8b2c6a14c62d197b510b3cc1076014c22d712c5ce\n",
            "Successfully built bleu\n",
            "Installing collected packages: efficiency, bleu\n",
            "Successfully installed bleu-0.3 efficiency-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "from bleu import multi_file_bleu, multi_list_bleu"
      ],
      "metadata": {
        "id": "Foe8nZtJH62f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for calculating the bleu scores\n",
        "def calc_bleu(model, name):\n",
        "  scores = []\n",
        "  for i in range(len(ref)):\n",
        "    scores.append(sentence_bleu(ref[i], model[i], weights = (1,0,0,0)))\n",
        "  score = sum(scores) / len(scores)\n",
        "  print(name, \"Bleu Score = \", score)\n",
        "  return score"
      ],
      "metadata": {
        "id": "zoLB7nncH64u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Bleu\n",
        "bleu_gpt3 = calc_bleu(gpt3, \"GPT3.5\")\n",
        "bleu_gpt4 = calc_bleu(gpt4, \"GPT4\")\n",
        "bleu_hindi = calc_bleu(hindi, \"Hindi GPT\")\n",
        "bleu_llama  = calc_bleu(llama, \"Llama3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDRkqNNwH663",
        "outputId": "6264d776-13ae-4dd9-f098-c419d1a1ea72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5 Bleu Score =  0.34779141198077984\n",
            "GPT4 Bleu Score =  0.3171056535698914\n",
            "Hindi GPT Bleu Score =  0.3578056465881177\n",
            "Llama3 Bleu Score =  0.3725709557162986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Bleurt**"
      ],
      "metadata": {
        "id": "eQZ-9h85HkOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing bluert\n",
        "!pip install --upgrade pip  # ensures that pip is current\n",
        "!git clone https://github.com/google-research/bleurt.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BLUUArQHHkqN",
        "outputId": "07de124d-d0e4-47c5-dced-40c0d98571c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n",
            "Cloning into 'bleurt'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 134 (delta 0), reused 17 (delta 0), pack-reused 116 (from 1)\u001b[K\n",
            "Receiving objects: 100% (134/134), 31.28 MiB | 18.81 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd bleurt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UneKDTWVLwe7",
        "outputId": "a6675686-b487-4956-da3b-1b2f17681a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bleurt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L_gMaGOQLwg-",
        "outputId": "3ff1ea58-a0ec-4504-f97c-70aee17465c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/bleurt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.13.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (2.17.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2024.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->BLEURT==0.0.2) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->BLEURT==0.0.2) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->BLEURT==0.0.2) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->BLEURT==0.0.2) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->BLEURT==0.0.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->BLEURT==0.0.2) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->BLEURT==0.0.2) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->BLEURT==0.0.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n",
            "Building wheels for collected packages: BLEURT\n",
            "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456764 sha256=eb4426ce95d95c5390e886670a4c7f4f6e62034cdb99bbc0ffbc3301ff7f5972\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xqkmalwt/wheels/92/4f/fb/afa555fa27aa9e2c7958df797a62cc4e74f0f459cec9c4fa7c\n",
            "Successfully built BLEURT\n",
            "Installing collected packages: BLEURT\n",
            "Successfully installed BLEURT-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bjf5A3I7Lwkl",
        "outputId": "8f0836bc-8c1c-42ca-8755-e159eb4c4f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-07 15:45:21--  https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.153.207, 142.250.145.207, 74.125.128.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.153.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2140294207 (2.0G) [application/octet-stream]\n",
            "Saving to: ‘BLEURT-20.zip’\n",
            "\n",
            "BLEURT-20.zip       100%[===================>]   1.99G  19.4MB/s    in 1m 54s  \n",
            "\n",
            "2024-11-07 15:47:16 (17.9 MB/s) - ‘BLEURT-20.zip’ saved [2140294207/2140294207]\n",
            "\n",
            "--2024-11-07 15:47:16--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2024-11-07 15:47:16--\n",
            "Total wall clock time: 1m 55s\n",
            "Downloaded: 1 files, 2.0G in 1m 54s (17.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip BLEURT-20.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Smnc_U-4L-LY",
        "outputId": "4737c17e-16a1-44ce-f96c-75fd848b9f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BLEURT-20.zip\n",
            "   creating: BLEURT-20/\n",
            "  inflating: BLEURT-20/bert_config.json  \n",
            "  inflating: BLEURT-20/saved_model.pb  \n",
            "   creating: BLEURT-20/variables/\n",
            "  inflating: BLEURT-20/variables/variables.index  \n",
            "  inflating: BLEURT-20/variables/variables.data-00000-of-00001  \n",
            "  inflating: BLEURT-20/sent_piece.vocab  \n",
            "  inflating: BLEURT-20/bleurt_config.json  \n",
            "  inflating: BLEURT-20/sent_piece.model  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bleurt.score_files \\\n",
        "  -candidate_file=bleurt/test_data/candidates \\\n",
        "  -reference_file=bleurt/test_data/references \\\n",
        "  -bleurt_checkpoint=BLEURT-20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9RkYaERVL-Nf",
        "outputId": "248b044e-8d06-4c98-cf4d-5e7eb5cf31be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-07 15:48:02.316500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-07 15:48:02.338847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-07 15:48:02.345419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-07 15:48:02.363019: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-07 15:48:03.866830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:tensorflow:Running BLEURT scoring.\n",
            "I1107 15:48:05.800114 137151336452096 score_files.py:168] Running BLEURT scoring.\n",
            "INFO:tensorflow:Reading checkpoint BLEURT-20.\n",
            "I1107 15:48:05.800422 137151336452096 score.py:160] Reading checkpoint BLEURT-20.\n",
            "INFO:tensorflow:Config file found, reading.\n",
            "I1107 15:48:05.800667 137151336452096 checkpoint.py:91] Config file found, reading.\n",
            "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
            "I1107 15:48:05.800940 137151336452096 checkpoint.py:95] Will load checkpoint BLEURT-20\n",
            "INFO:tensorflow:Loads full paths and checks that files exists.\n",
            "I1107 15:48:05.801048 137151336452096 checkpoint.py:97] Loads full paths and checks that files exists.\n",
            "INFO:tensorflow:... name:BLEURT-20\n",
            "I1107 15:48:05.801147 137151336452096 checkpoint.py:101] ... name:BLEURT-20\n",
            "INFO:tensorflow:... bert_config_file:bert_config.json\n",
            "I1107 15:48:05.801225 137151336452096 checkpoint.py:101] ... bert_config_file:bert_config.json\n",
            "INFO:tensorflow:... max_seq_length:512\n",
            "I1107 15:48:05.801354 137151336452096 checkpoint.py:101] ... max_seq_length:512\n",
            "INFO:tensorflow:... vocab_file:None\n",
            "I1107 15:48:05.801435 137151336452096 checkpoint.py:101] ... vocab_file:None\n",
            "INFO:tensorflow:... do_lower_case:None\n",
            "I1107 15:48:05.801536 137151336452096 checkpoint.py:101] ... do_lower_case:None\n",
            "INFO:tensorflow:... sp_model:sent_piece\n",
            "I1107 15:48:05.801606 137151336452096 checkpoint.py:101] ... sp_model:sent_piece\n",
            "INFO:tensorflow:... dynamic_seq_length:True\n",
            "I1107 15:48:05.801717 137151336452096 checkpoint.py:101] ... dynamic_seq_length:True\n",
            "INFO:tensorflow:Creating BLEURT scorer.\n",
            "I1107 15:48:05.801803 137151336452096 score.py:167] Creating BLEURT scorer.\n",
            "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
            "I1107 15:48:05.801877 137151336452096 tokenizers.py:79] Creating SentencePiece tokenizer.\n",
            "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
            "I1107 15:48:05.801949 137151336452096 tokenizers.py:58] Creating SentencePiece tokenizer.\n",
            "INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n",
            "I1107 15:48:05.802024 137151336452096 tokenizers.py:60] Will load model: BLEURT-20/sent_piece.model.\n",
            "INFO:tensorflow:SentencePiece tokenizer created.\n",
            "I1107 15:48:06.545894 137151336452096 tokenizers.py:64] SentencePiece tokenizer created.\n",
            "INFO:tensorflow:Creating Eager Mode predictor.\n",
            "I1107 15:48:06.546209 137151336452096 score.py:56] Creating Eager Mode predictor.\n",
            "INFO:tensorflow:Loading model.\n",
            "I1107 15:48:06.546337 137151336452096 score.py:61] Loading model.\n",
            "2024-11-07 15:48:10.359015: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 256307200 exceeds 10% of free system memory.\n",
            "2024-11-07 15:48:10.423233: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2024-11-07 15:48:10.487782: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2024-11-07 15:48:10.599553: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "2024-11-07 15:48:10.659191: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21233664 exceeds 10% of free system memory.\n",
            "I1107 15:48:17.303114 137151336452096 load.py:1083] Fingerprint not found. Saved model loading will continue.\n",
            "I1107 15:48:17.303571 137151336452096 load.py:1102] path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "INFO:tensorflow:BLEURT initialized.\n",
            "I1107 15:48:17.360756 137151336452096 score.py:173] BLEURT initialized.\n",
            "INFO:tensorflow:Computing BLEURT scores...\n",
            "I1107 15:48:17.361030 137151336452096 score_files.py:132] Computing BLEURT scores...\n",
            "INFO:tensorflow:BLEURT scores computed.\n",
            "I1107 15:49:20.002808 137151336452096 score_files.py:140] BLEURT scores computed.\n",
            "0.9777957797050476\n",
            "0.7689993977546692\n",
            "0.4400966763496399\n",
            "0.16677355766296387\n",
            "INFO:tensorflow:Done.\n",
            "I1107 15:49:20.003123 137151336452096 score_files.py:150] Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from bleurt import score"
      ],
      "metadata": {
        "id": "Rqf8GPKAMpk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bleurt(answers, name):\n",
        "  bleurt_scores = scorer.score(references=ref, candidates=answers)\n",
        "  score = sum(bleurt_scores) / len(bleurt_scores)\n",
        "  print(name, \"Bleurt Score = \", score)\n",
        "  return score"
      ],
      "metadata": {
        "id": "3o5SnwpZL-RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Bleurt Scores\n",
        "checkpoint = \"bleurt/test_checkpoint\"\n",
        "scorer = score.BleurtScorer(checkpoint)\n",
        "\n",
        "bleurt_gpt3 = calc_bleurt(gpt3, \"GPT3.5\")\n",
        "bleurt_gpt4 = calc_bleurt(gpt4, \"GPT4\")\n",
        "bleurt_hindi = calc_bleurt(hindi, \"Hindi GPT\")\n",
        "bleurt_llama  = calc_bleurt(llama, \"Llama3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXXphJkMHW8",
        "outputId": "08e927cf-e9b3-4974-aa3d-390ea04f251a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5 Bleurt Score =  0.5296638231724501\n",
            "GPT4 Bleurt Score =  0.43066286779940127\n",
            "Hindi GPT Bleurt Score =  0.4971300791949034\n",
            "Llama3 Bleurt Score =  0.4578865200281143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Meteor**"
      ],
      "metadata": {
        "id": "Fgv7i-8nHsxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nltk\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX_L6nYE3fFr",
        "outputId": "e07a33b4-9bce-4f22-bcb7-c08adffe04fe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0bb93c-17dc-4bbf-c22d-5c7808d2dd70",
        "collapsed": true,
        "id": "OrPFDt5bNPPr"
      },
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "from csv import DictWriter\n",
        "from nltk.translate.meteor_score import single_meteor_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for calculating the meteor scores\n",
        "def calc_meteor(answers, name):\n",
        "  scores = []\n",
        "  for i in range(len(ref)):\n",
        "    scores.append(round(single_meteor_score(word_tokenize(ref[i]), word_tokenize(answers[i])), 4))\n",
        "  score = sum(scores) / len(scores)\n",
        "  print(name, 'Meteor Scores = ', score)\n",
        "  return score"
      ],
      "metadata": {
        "id": "Xp-oyYuOHrvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Meteor Scores\n",
        "meteor_gpt3 = calc_meteor(gpt3, \"GPT3.5\")\n",
        "meteor_gpt4 = calc_meteor(gpt4, \"GPT4\")\n",
        "meteor_hindi = calc_meteor(hindi, \"Hindi GPT\")\n",
        "meteor_llama  = calc_meteor(llama, \"Llama3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpyZxSydNkxW",
        "outputId": "ebe08069-8870-46a5-c07b-78870d3fc398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5 Meteor Scores =  0.515045\n",
            "GPT4 Meteor Scores =  0.5161150000000001\n",
            "Hindi GPT Meteor Scores =  0.5074575\n",
            "Llama3 Meteor Scores =  0.5079175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Cosine Similarity using FastText**"
      ],
      "metadata": {
        "id": "jZquhRUkWJCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jyH8977WWIRS",
        "outputId": "8f7df66d-f442-408a-fc92-56a1d1108fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296188 sha256=956463ede744d8075060935cfcae98eb38364e46889333fb22e008cc244d91f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "2tUWDuYTXMTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_path = \"/content/gdrive/MyDrive/Colab Notebooks/Hindi Comprehension/Answer Extraction/\""
      ],
      "metadata": {
        "id": "YhATSLL6WfiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "ft = fasttext.load_model(vector_path + 'wiki.hi.bin')"
      ],
      "metadata": {
        "id": "zUX1af_hUw8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
        "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
        "    if not magnitude:\n",
        "        return 0\n",
        "    return dot_product/magnitude"
      ],
      "metadata": {
        "id": "GTHcR1aEW2It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function for calculating the meteor scores\n",
        "def calc_cos(answers, name):\n",
        "  scores = []\n",
        "  for i in range(len(ref)):\n",
        "    scores.append(cosine_similarity(ft.get_sentence_vector(ref[i].replace('\\n', ' ')), ft.get_sentence_vector(answers[i].replace('\\n', ' '))))\n",
        "  score = sum(scores) / len(scores)\n",
        "  print(name, 'Cosine Similarity = ', score)\n",
        "  return score"
      ],
      "metadata": {
        "id": "Se04D_7PXWZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Meteor Scores\n",
        "cos_gpt3 = calc_cos(gpt3, \"GPT3.5\")\n",
        "cos_gpt4 = calc_cos(gpt4, \"GPT4\")\n",
        "cos_hindi = calc_cos(hindi, \"Hindi GPT\")\n",
        "cos_llama  = calc_cos(llama, \"Llama3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uR375_gW3kR",
        "outputId": "8635aaa0-d7ff-4ec7-caff-f875367151e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5 Cosine Similarity =  0.9223463013718695\n",
            "GPT4 Cosine Similarity =  0.9233784419644875\n",
            "Hindi GPT Cosine Similarity =  0.9225622604946482\n",
            "Llama3 Cosine Similarity =  0.9139167658985897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **End**\n",
        "---"
      ],
      "metadata": {
        "id": "fXCW76LDOHjC"
      }
    }
  ]
}